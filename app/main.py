import os
import shutil
import tarfile
import datetime
import logging
import json
import subprocess
import base64
import hashlib
import secrets
from typing import List, Dict, Optional

# ç¬¬ä¸‰æ–¹åº“
import httpx
from fastapi import FastAPI, UploadFile, BackgroundTasks, HTTPException, File, Depends, status
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.security import HTTPBasic, HTTPBasicCredentials
from fastapi.middleware.cors import CORSMiddleware
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.cron import CronTrigger
from webdav4.client import Client as WebDavClient
from cryptography.fernet import Fernet
from pytz import timezone

# --- å…¨å±€é…ç½®ä¸å¸¸é‡ ---

DATA_DIR = "/data"
CONF_DIR = "/conf"
BACKUP_CONFIG_FILE = os.path.join(CONF_DIR, "backup_config.json")
LOG_FILE = os.path.join(CONF_DIR, "manager.log")
TEMP_DIR = "/tmp/backup_work"
TZ_CN = timezone('Asia/Shanghai')

# è¯»å–ç¯å¢ƒå˜é‡ä¸­çš„ç®¡ç†å‘˜è´¦å·å¯†ç ï¼Œé»˜è®¤ä¸º admin/admin
ADMIN_USER = os.getenv("DASHBOARD_ADMIN_USER", "admin")
ADMIN_PASS = os.getenv("DASHBOARD_ADMIN_PASSWORD", "admin")

# ç¡®ä¿å¿…è¦ç›®å½•å­˜åœ¨
os.makedirs(CONF_DIR, exist_ok=True)
os.makedirs(TEMP_DIR, exist_ok=True)

# æ—¥å¿—é…ç½®
logging.basicConfig(
    filename=LOG_FILE,
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
# åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°ä»¥ä¾¿ docker logs æŸ¥çœ‹
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
logging.getLogger().addHandler(console_handler)

app = FastAPI(title="Vaultwarden Dashboard")
security = HTTPBasic()

# å…è®¸è·¨åŸŸ
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- é‰´æƒå‡½æ•° ---

def check_auth(credentials: HTTPBasicCredentials = Depends(security)):
    """éªŒè¯ç”¨æˆ·åå’Œå¯†ç """
    is_user_correct = secrets.compare_digest(credentials.username, ADMIN_USER)
    is_pass_correct = secrets.compare_digest(credentials.password, ADMIN_PASS)
    
    if not (is_user_correct and is_pass_correct):
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Incorrect username or password",
            headers={"WWW-Authenticate": "Basic"},
        )
    return credentials.username

# --- è¾…åŠ©åŠŸèƒ½å‡½æ•° ---

def load_config() -> dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    if os.path.exists(BACKUP_CONFIG_FILE):
        try:
            with open(BACKUP_CONFIG_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logging.error(f"åŠ è½½é…ç½®å¤±è´¥: {e}")
    return {}

def save_config(config: dict):
    """ä¿å­˜é…ç½®æ–‡ä»¶ï¼Œå¹¶å°è¯•æ›´æ–°è°ƒåº¦ä»»åŠ¡"""
    with open(BACKUP_CONFIG_FILE, 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=4, ensure_ascii=False)
    
    # å°è¯•é‡æ–°è°ƒåº¦
    try:
        schedule_backup_job(config)
    except Exception as e:
        logging.error(f"æ›´æ–°è°ƒåº¦ä»»åŠ¡å¤±è´¥: {e}")

def get_current_time_str():
    """è·å–å½“å‰åŒ—äº¬æ—¶é—´å­—ç¬¦ä¸²"""
    return datetime.datetime.now(TZ_CN).strftime("%Y%m%d_%H%M%S")

def send_telegram_notify(msg: str, success: bool = True):
    """å‘é€ Telegram é€šçŸ¥"""
    cfg = load_config()
    token = cfg.get("tg_bot_token")
    chat_id = cfg.get("tg_chat_id")
    
    if not token or not chat_id:
        return
    
    # ã€ä¿®æ”¹ç‚¹ã€‘å¦‚æœæ˜¯æˆåŠŸæ¶ˆæ¯ï¼Œä¸”ä¸åœ¨è°ƒè¯•æ¨¡å¼ä¸‹ï¼Œå¯ä»¥é€‰æ‹©ä¸å‘é€
    # ä½†ç”±äºéœ€æ±‚æ˜¯â€œä»…å¤±è´¥å‘é€â€ï¼Œæˆ‘ä»¬åœ¨è°ƒç”¨ç«¯æ§åˆ¶ï¼Œè¿™é‡Œåªè´Ÿè´£å‘
    
    emoji = "âœ…" if success else "âŒ"
    title = "Vaultwarden å¤‡ä»½æˆåŠŸ" if success else "Vaultwarden å¤‡ä»½/è¿˜åŸå¤±è´¥"
    text = f"{emoji} *{title}*\n\n{msg}\n\nğŸ•’ æ—¶é—´: {datetime.datetime.now(TZ_CN).strftime('%Y-%m-%d %H:%M:%S')}"
    
    url = f"https://api.telegram.org/bot{token}/sendMessage"
    try:
        httpx.post(url, json={"chat_id": chat_id, "text": text, "parse_mode": "Markdown"}, timeout=10)
    except Exception as e:
        logging.error(f"Telegram å‘é€å¤±è´¥: {e}")

def get_fernet_key(password: str) -> bytes:
    """æ ¹æ®å¯†ç ç”Ÿæˆå›ºå®šçš„ AES Key"""
    digest = hashlib.sha256(password.encode()).digest()
    return base64.urlsafe_b64encode(digest)

def encrypt_file(file_path: str, password: str) -> str:
    key = get_fernet_key(password)
    fernet = Fernet(key)
    with open(file_path, 'rb') as f:
        data = f.read()
    encrypted_data = fernet.encrypt(data)
    out_path = file_path + ".enc"
    with open(out_path, 'wb') as f:
        f.write(encrypted_data)
    return out_path

def decrypt_file(file_path: str, password: str) -> str:
    key = get_fernet_key(password)
    fernet = Fernet(key)
    with open(file_path, 'rb') as f:
        data = f.read()
    decrypted_data = fernet.decrypt(data)
    out_path = file_path.replace(".enc", "")
    with open(out_path, 'wb') as f:
        f.write(decrypted_data)
    return out_path

# --- ä¿ç•™ç­–ç•¥é€»è¾‘ (å·²ä¿®å¤ WebDAV è·¯å¾„æ‹¼æ¥é—®é¢˜) ---

def apply_retention_policy(client: WebDavClient, remote_dir: str):
    """åº”ç”¨ä¿ç•™ç­–ç•¥ï¼šä¿ç•™æœ€æ–°çš„ N ä¸ªå¤‡ä»½ï¼Œåˆ é™¤æ—§çš„"""
    cfg = load_config()
    max_backups = int(cfg.get("max_backups", 10))
    if max_backups < 1: max_backups = 10

    try:
        files = client.ls(remote_dir, detail=True)
        backups = []
        
        for f in files:
            if f['type'] == 'directory':
                continue
            
            # WebDAV ls è¿”å›çš„ f['name'] é€šå¸¸åŒ…å«å®Œæ•´è·¯å¾„ (ä¾‹å¦‚ /folder/file.tar.gz)
            # æˆ‘ä»¬åªç”¨ basename æ¥åˆ¤æ–­æ˜¯ä¸æ˜¯å¤‡ä»½æ–‡ä»¶
            name = os.path.basename(f['name'])
            
            if "vw_backup_" in name:
                backups.append({
                    "name": name, 
                    "path": f['name'], # ã€å…³é”®ã€‘ä¿ç•™ ls è¿”å›çš„åŸå§‹è·¯å¾„ç”¨äºåˆ é™¤
                    "sort_key": name 
                })
        
        # æŒ‰åç§°é™åº (æœ€æ–°åœ¨æœ€å‰)
        backups.sort(key=lambda x: x['sort_key'], reverse=True)
        
        logging.info(f"æ£€æŸ¥ä¿ç•™ç­–ç•¥: å½“å‰æœ‰ {len(backups)} ä¸ªå¤‡ä»½, é™åˆ¶ä¸º {max_backups}")

        if len(backups) > max_backups:
            to_delete = backups[max_backups:]
            
            for item in to_delete:
                # ã€ä¿®å¤ã€‘ç›´æ¥ä½¿ç”¨ ls è¿”å›çš„è·¯å¾„ï¼Œä¸è¦é‡å¤æ‹¼æ¥ remote_dir
                path_to_remove = item['path']
                
                logging.info(f"æ­£åœ¨åˆ é™¤è¿‡æœŸå¤‡ä»½: {path_to_remove}")
                try:
                    client.remove(path_to_remove)
                except Exception as ex:
                    # å¦‚æœç›´æ¥åˆ é™¤å¤±è´¥ï¼Œå°è¯•åŠ å‰å¯¼æ–œæ ï¼ˆé’ˆå¯¹æŸäº›ç‰¹æ®Šçš„ WebDAV æœåŠ¡ç«¯ï¼‰
                    logging.warning(f"åˆ é™¤å¤±è´¥ ({ex})ï¼Œå°è¯•ä¿®æ­£è·¯å¾„é‡è¯•...")
                    try:
                        if not path_to_remove.startswith('/'):
                            client.remove('/' + path_to_remove)
                        else:
                            client.remove(path_to_remove.lstrip('/'))
                    except Exception as ex2:
                        logging.error(f"å½»åº•æ— æ³•åˆ é™¤æ–‡ä»¶ {path_to_remove}: {ex2}")
            
            logging.info(f"æ¸…ç†å®Œæˆï¼Œå…±åˆ é™¤äº† {len(to_delete)} ä¸ªæ—§æ–‡ä»¶ã€‚")
            
    except Exception as e:
        logging.error(f"ä¿ç•™ç­–ç•¥æ¸…ç†è¿‡ç¨‹å‡ºé”™: {e}")

# --- æ ¸å¿ƒå¤‡ä»½é€»è¾‘ ---

def perform_backup():
    """æ‰§è¡Œå®Œæ•´çš„å¤‡ä»½æµç¨‹"""
    logging.info("å¼€å§‹æ‰§è¡Œå®šæ—¶å¤‡ä»½ä»»åŠ¡...")
    cfg = load_config()
    
    if not cfg.get("webdav_url"):
        logging.warning("æœªé…ç½® WebDAVï¼Œè·³è¿‡å¤‡ä»½ã€‚")
        return

    tmp_files = []
    backup_name = ""

    try:
        timestamp = get_current_time_str()
        backup_name = f"vw_backup_{timestamp}.tar.gz"
        tar_path = os.path.join(TEMP_DIR, backup_name)
        tmp_files.append(tar_path)

        # 1. å¤‡ä»½ SQLite
        sqlite_db_path = os.path.join(DATA_DIR, "db.sqlite3")
        backup_db_path = os.path.join(TEMP_DIR, "db.sqlite3")
        
        if os.path.exists(sqlite_db_path):
            logging.info("æ­£åœ¨å¯¼å‡º SQLite æ•°æ®åº“...")
            subprocess.run(["sqlite3", sqlite_db_path, f".backup '{backup_db_path}'"], check=True)
            tmp_files.append(backup_db_path)

        # 2. æ‰“åŒ…
        logging.info("æ­£åœ¨æ‰“åŒ…æ–‡ä»¶...")
        with tarfile.open(tar_path, "w:gz") as tar:
            if os.path.exists(backup_db_path):
                tar.add(backup_db_path, arcname="db.sqlite3")
            for item in ["attachments", "sends", "rsa_key.pem", "rsa_key.pub.pem", "config.json", "data.json", "icon_cache"]:
                p = os.path.join(DATA_DIR, item)
                if os.path.exists(p):
                    tar.add(p, arcname=item)
        
        # 3. åŠ å¯†
        upload_path = tar_path
        if cfg.get("encryption_password"):
            logging.info("æ­£åœ¨åŠ å¯†å¤‡ä»½æ–‡ä»¶...")
            upload_path = encrypt_file(tar_path, cfg["encryption_password"])
            tmp_files.append(upload_path)
            backup_name += ".enc"

        # 4. ä¸Šä¼ 
        logging.info(f"æ­£åœ¨ä¸Šä¼ åˆ° WebDAV: {cfg['webdav_url']}")
        client = WebDavClient(
            cfg["webdav_url"], 
            auth=(cfg.get("webdav_user", ""), cfg.get("webdav_password", ""))
        )
        
        remote_dir = cfg.get('webdav_path', '/')
        try:
            if remote_dir != "/":
                if not client.exists(remote_dir):
                    client.mkdir(remote_dir)
        except Exception as e:
             logging.warning(f"å°è¯•åˆ›å»ºç›®å½•å¤±è´¥: {e}")

        remote_path = f"{remote_dir}/{backup_name}".replace("//", "/")
        client.upload_file(upload_path, remote_path)
        logging.info("ä¸Šä¼ æˆåŠŸã€‚")
        
        # 5. ä¿ç•™ç­–ç•¥
        logging.info("æ­£åœ¨æ£€æŸ¥ä¿ç•™ç­–ç•¥...")
        apply_retention_policy(client, remote_dir)

        # ã€ä¿®æ”¹ã€‘æˆåŠŸæ—¶ä¸å‘é€é€šçŸ¥ï¼Œä»…è®°å½•æ—¥å¿—
        logging.info(f"å¤‡ä»½æµç¨‹å…¨éƒ¨å®Œæˆ: {backup_name}")

    except Exception as e:
        logging.error(f"å¤‡ä»½æµç¨‹å¤±è´¥: {e}", exc_info=True)
        # ã€ä¿®æ”¹ã€‘ä»…å¤±è´¥æ—¶å‘é€é€šçŸ¥
        send_telegram_notify(f"å¤‡ä»½æµç¨‹å‘ç”Ÿå¼‚å¸¸: {str(e)}", success=False)
    finally:
        for f in tmp_files:
            if os.path.exists(f):
                try:
                    if os.path.isdir(f): shutil.rmtree(f)
                    else: os.remove(f)
                except: pass

# --- è¿˜åŸé€»è¾‘ ---

def restart_vaultwarden():
    logging.info("æ­£åœ¨é‡å¯ Vaultwarden...")
    try:
        # ç¡®ä¿ supervisorctl ä½¿ç”¨ sock æ–‡ä»¶é…ç½® (å‚è€ƒä¹‹å‰çš„ supervisord.conf ä¿®æ”¹)
        subprocess.run(["supervisorctl", "restart", "vaultwarden"], check=True)
        logging.info("Vaultwarden é‡å¯å‘½ä»¤å·²å‘é€ã€‚")
    except subprocess.CalledProcessError as e:
        logging.error(f"é‡å¯ Vaultwarden å¤±è´¥: {e}")
        raise e

def process_restore_file(local_file_path: str):
    cfg = load_config()
    temp_restored_files = []
    
    try:
        work_file = local_file_path

        # 1. è§£å¯†
        if local_file_path.endswith(".enc"):
            if not cfg.get("encryption_password"):
                raise ValueError("æ–‡ä»¶å·²åŠ å¯†ï¼Œä½†æœªé…ç½®è§£å¯†å¯†ç ï¼")
            logging.info("æ­£åœ¨è§£å¯†æ–‡ä»¶...")
            work_file = decrypt_file(local_file_path, cfg["encryption_password"])
            temp_restored_files.append(work_file)
        
        # 2. è§£å‹
        logging.info("æ­£åœ¨è§£å‹è¦†ç›–æ•°æ®...")
        if not tarfile.is_tarfile(work_file):
            raise ValueError("æ–‡ä»¶ä¸æ˜¯æœ‰æ•ˆçš„ tar å½’æ¡£")

        subprocess.run(["supervisorctl", "stop", "vaultwarden"], check=False)

        with tarfile.open(work_file, "r:gz") as tar:
            tar.extractall(path=DATA_DIR)
        
        logging.info("æ•°æ®è¦†ç›–å®Œæˆã€‚")

        # 3. é‡å¯
        restart_vaultwarden()
        # ã€ä¿®æ”¹ã€‘æˆåŠŸæ—¶ä¸å‘é€é€šçŸ¥
        logging.info("ç³»ç»Ÿå·²æˆåŠŸä»å¤‡ä»½è¿˜åŸå¹¶é‡å¯ã€‚")

    except Exception as e:
        logging.error(f"è¿˜åŸå¤±è´¥: {e}", exc_info=True)
        # ã€ä¿®æ”¹ã€‘ä»…å¤±è´¥æ—¶å‘é€é€šçŸ¥
        send_telegram_notify(f"è¿˜åŸæ“ä½œå¤±è´¥: {str(e)}", success=False)
        subprocess.run(["supervisorctl", "start", "vaultwarden"], check=False)
    finally:
        if os.path.exists(local_file_path): os.remove(local_file_path)
        for f in temp_restored_files:
            if os.path.exists(f): os.remove(f)

def download_and_restore(filename: str):
    cfg = load_config()
    local_filename = os.path.basename(filename)
    local_path = os.path.join(TEMP_DIR, local_filename)
    
    try:
        logging.info(f"å¼€å§‹ä¸‹è½½å¤‡ä»½æ–‡ä»¶: {filename}")
        client = WebDavClient(
            cfg["webdav_url"], 
            auth=(cfg.get("webdav_user", ""), cfg.get("webdav_password", ""))
        )
        
        # è¿™é‡Œçš„ filename å¯èƒ½æ˜¯å‰ç«¯ä¼ æ¥çš„çº¯æ–‡ä»¶åï¼Œä¹Ÿå¯èƒ½æ˜¯ list_backups è¿”å›çš„
        # ä¸ºäº†ä¿é™©ï¼Œæˆ‘ä»¬é‡æ–°æ‹¼è£…è¿œç¨‹è·¯å¾„
        remote_path = f"{cfg.get('webdav_path', '/')}/{local_filename}".replace("//", "/")
        
        client.download_file(remote_path, local_path)
        process_restore_file(local_path)
    except Exception as e:
        logging.error(f"ä¸‹è½½/è¿˜åŸè¿‡ç¨‹å‡ºé”™: {e}")
        send_telegram_notify(f"ä¸‹è½½/è¿˜åŸè¿‡ç¨‹å‡ºé”™: {e}", success=False)

# --- è°ƒåº¦å™¨è®¾ç½® ---

scheduler = BackgroundScheduler(timezone=TZ_CN)

def schedule_backup_job(config: dict):
    if scheduler.get_job('backup_job'):
        scheduler.remove_job('backup_job')
    
    cron_exp = config.get('schedule_cron', '0 3 * * *')
    
    try:
        trigger = CronTrigger.from_crontab(cron_exp, timezone=TZ_CN)
        scheduler.add_job(
            perform_backup, 
            trigger, 
            id='backup_job',
            replace_existing=True
        )
        logging.info(f"å¤‡ä»½ä»»åŠ¡å·²æ›´æ–°ï¼ŒCron: {cron_exp}")
    except ValueError as e:
        logging.error(f"Cron è¡¨è¾¾å¼é”™è¯¯: {cron_exp}, ä½¿ç”¨é»˜è®¤å€¼")
        scheduler.add_job(
            perform_backup, 
            CronTrigger(hour=3, minute=0, timezone=TZ_CN), 
            id='backup_job',
            replace_existing=True
        )

scheduler.start()
initial_cfg = load_config()
schedule_backup_job(initial_cfg)


# --- API è·¯ç”± ---

@app.get("/", response_class=HTMLResponse)
async def read_root():
    index_path = "/app/static/index.html"
    if not os.path.exists(index_path): index_path = "app/static/index.html"
    if os.path.exists(index_path):
        with open(index_path, "r", encoding="utf-8") as f: return f.read()
    return "UI File Not Found."

@app.get("/api/auth_check", dependencies=[Depends(check_auth)])
async def auth_check():
    return {"status": "authenticated"}

@app.get("/api/config", dependencies=[Depends(check_auth)])
async def get_config():
    return load_config()

@app.post("/api/config", dependencies=[Depends(check_auth)])
async def update_config(config: dict):
    save_config(config)
    return {"status": "success"}

@app.post("/api/backup/now", dependencies=[Depends(check_auth)])
async def trigger_backup_manual(background_tasks: BackgroundTasks):
    background_tasks.add_task(perform_backup)
    return {"status": "started"}

@app.get("/api/backups", dependencies=[Depends(check_auth)])
async def list_backups():
    cfg = load_config()
    if not cfg.get("webdav_url"):
        return JSONResponse(status_code=400, content={"error": "WebDAV not configured"})
    
    try:
        client = WebDavClient(
            cfg["webdav_url"], 
            auth=(cfg.get("webdav_user", ""), cfg.get("webdav_password", ""))
        )
        # detail=True è·å–å®Œæ•´ä¿¡æ¯
        files = client.ls(cfg.get('webdav_path', '/'), detail=True)
        
        backup_files = []
        for f in files:
            if f.get('type') != 'directory' and "vw_backup_" in f.get('name', ''):
                clean_name = os.path.basename(f['name'])
                size_mb = round(int(f.get('size', 0)) / 1024 / 1024, 2)
                backup_files.append({
                    "name": clean_name,
                    "size": f"{size_mb} MB",
                    "last_modified": f.get('last_modified', '')
                })
        
        return sorted(backup_files, key=lambda x: x['name'], reverse=True)
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})

@app.post("/api/restore", dependencies=[Depends(check_auth)])
async def restore_from_cloud(file_name: str, background_tasks: BackgroundTasks):
    background_tasks.add_task(download_and_restore, file_name)
    return {"status": "started"}

@app.post("/api/upload_restore", dependencies=[Depends(check_auth)])
async def upload_and_restore(background_tasks: BackgroundTasks, file: UploadFile = File(...)):
    local_path = os.path.join(TEMP_DIR, file.filename)
    try:
        with open(local_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        background_tasks.add_task(process_restore_file, local_path)
        return {"status": "started"}
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})

@app.get("/api/logs", dependencies=[Depends(check_auth)])
async def get_logs():
    if os.path.exists(LOG_FILE):
        try:
            with open(LOG_FILE, 'r', encoding='utf-8') as f:
                return {"logs": "".join(f.readlines()[-100:])}
        except: pass
    return {"logs": "No logs yet."}
